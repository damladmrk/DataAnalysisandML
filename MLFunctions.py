# -*- coding: utf-8 -*-
"""MLFunctions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tqS5TM6BS15f_OPRPf_2GCMk9ESFhf_z

# **Libraries**
"""

import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score

"""# **Data Preprocessing**"""

def check_df(dataframe, head=5):
    """

    It gives an overview for a data about it's columns, their quantiles etc.

    Parameters
    ------
        dataframe: dataframe
                Dataframe that will be analyzed.
        head=5: int, optional
                numeric threshold to be set for categoric variables
    Returns
    ------
        None

    """
    print("______________ Shape ______________") # Removed extra space before this line
    print(dataframe.shape)
    print("______________ Types ______________")
    print(dataframe.dtypes)
    print("______________ Head ______________")
    print(dataframe.head(head))
    print("______________ Tail ______________")
    print(dataframe.tail(head))
    print("______________ Nulls ______________")
    print(dataframe.isnull().sum())
    print("______________ Quantiles ______________")
    print(dataframe.quantile([0, 0.25, 0.50, 0.75, 0.99, 1]).T)
    print("______________ Bools&Obj ______________")
    print(dataframe.describe(include=["object", "bool"]))

def column_names(dataframe, cat_th=10, car_th=20):
    """

    It seperates the columns to their types as categorical and cardinal.
    Note: It holds numeric variables in the categorical part if they are under the treshold.

    Parameters
    ------
        dataframe: dataframe
                Dataframe that will be analyzed.
        cat_th: int, optional
                numeric threshold to be set for categoric variables
        car_th: int, optional
                numeric threshold to be set for cardinal variables
    Returns
    ------
        cat_cols: list
                Categoric variable list
        num_cols: list
                Numeric variable list
        cat_but_car: list
                Categoric to cardinal variable list

    Notes
    ------
        cat_cols + num_cols + cat_but_car = total variable list
        num_but_cat is in cat_cols

    """
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtypes == "O"]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]

    print(f"Observations: {dataframe.shape[0]}")
    print(f"Variables: {dataframe.shape[1]}")
    print(f'cat_cols: {len(cat_cols)}')
    print(f'num_cols: {len(num_cols)}')
    print(f'cat_but_car: {len(cat_but_car)}')
    print(f'num_but_cat: {len(num_but_cat)}')

    return cat_cols, num_cols, cat_but_car

def cat_summ(dataframe, plot=False):
  """
      It gives an overview in categoric columns in a given dataframe.

      Parameters
      ------
          dataframe: dataframe
                  Dataframe that will be analyzed.
          plot=False: bool, optional
                  numeric threshold to be set for categoric variables
      Returns
      ------
          None
  """
  cat_cols, num_cols, cat_but_car = olumn_names(dataframe)
  for col_name in cat_cols:
    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(sort=True),
                        "Ratio": 100 * dataframe[col_name].value_counts() / len(dataframe)}))
    print("__________________________")
    if plot:
        sns.countplot(x=dataframe[col_name], data=dataframe)
        plt.show()

def num_summ(dataframe, plot=False):
  """
      It gives an overview in numeric columns in a given dataframe.

      Parameters
      ------
          dataframe: dataframe
                  Dataframe that will be analyzed.
          plot=False: bool, optional
                  numeric threshold to be set for numeric variables
      Returns
      ------
          None
  """
  quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]
  cat_cols, num_cols, cat_but_car = column_names(dataframe)
  for col_name in num_cols:
      print(dataframe[col_name].describe(quantiles).T)

      if plot:
          dataframe[col_name].hist(bins=20)
          plt.xlabel(col_name)
          plt.title(col_name)
          plt.show()

def target_summary_with_num(dataframe, target, numerical_col):
  """

    It seperates the numerical column by grouping the target column's values.

    Parameters
    ------
        dataframe: dataframe
                Dataframe that will be analyzed.
        target: str
                Target variable
        numerical_col: str
                Numerical variable
    Returns
    ------
        None

    """
  print(dataframe.groupby(target).agg({numerical_col: "mean"}), end="\n\n\n")

def corr_matrix(dataframe, plot=False):
  """
      It shows correlations of variables in a given dataframe.

      Parameters
      ------
          dataframe: dataframe
                  Dataframe that will be analyzed.
          plot=False: bool, optional
                  if True, it plots the heatmap
      Returns
      ------
          None
  """
  print(dataframe.corr())
  if plot:
    f, ax = plt.subplots(figsize=[8, 5])
    sns.heatmap(dataframe.corr(), annot=True, fmt=".2f", ax=ax, cmap="magma")
    ax.set_title("Correlation Matrix", fontsize=10)
    plt.show()

def train_test_split(dataframe, target, test_size= 20, random_state= 7):
  """

    It seperates the data into train and test sets due to giver params.

    Parameters
    ------
        dataframe: dataframe
                Dataframe that will be analyzed.
        target: str
                Target variable
        test_size: int, optional
                percentage of test set
        random_state: int, optional
                given number to take same sets in each iteration
    Returns
    ------
        X_train, X_test, y_train, y_test : train and test sets

    """
  y = dataframe[target]
  X = dataframe.drop(target, axis=1)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size, random_state)
  return X_train, X_test, y_train, y_test

def plot_importance(model, features, num=None, save=False):
    """
    Plots feature importances of a trained model.
    """

    if num is None:
        num = len(features.columns)

    feature_imp = pd.DataFrame({
        'Value': model.feature_importances_,
        'Feature': features.columns
    })

    feature_imp = feature_imp.sort_values(by="Value", ascending=False).head(num)

    plt.figure(figsize=(10, 10))
    sns.set(font_scale=1)

    sns.barplot(x="Value", y="Feature", data=feature_imp)

    plt.title('Feature Importances')
    plt.tight_layout()

    if save:
        plt.savefig('importances.png', dpi=300)

    plt.show()

def null_values_table(dataframe, na_name=False):
  """
    It shows the ratio of naN values in a given data.

    Parameters
    ------
      dataframe: Pandas dataframe

      na_name: default= False
        If True, it returns the columns that include naN values

    Returns
    ------
      na_columns: list
        If True, columns that include naN values

  """

  na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]
  n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)
  ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)

  missing_df = pd.DataFrame({
      'n_miss': n_miss,
      'ratio': np.round(ratio, 2)
  })

  print(missing_df, end="\n")
  if na_name:
     return na_columns

def replace_outliers_with_limits(dataframe, column_name, q1= 0.2, q3= 0.8):
    """
    Replace outliers in a column with the upper and lower limits by using IQR approach.

    Parameters:
    ------
        dataframe : pd.DataFrame
            dataframe (pandas DataFrame).

        column_name : str
            target column to change.

        q1 : float, default= 0.2
            quantile for lower limit.

        q3 : float, default= 0.8
            quantile for upper limit.

    Returns:
    ------
        pd.DataFrame
            Dataframe with outliers replaced.

        float
            ratio of values lies outside the lower limit.

        float
            ratio of values lies outside the upper limit.
    """

    Q1 = dataframe[column_name].quantile(q1)
    Q3 = dataframe[column_name].quantile(q3)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    lower_outliers = dataframe[column_name] < lower_bound
    upper_outliers = dataframe[column_name] > upper_bound
    lower_outlier_ratio = lower_outliers.sum() / len(dataframe)
    upper_outlier_ratio = upper_outliers.sum() / len(dataframe)

    dataframe[column_name] = np.where(dataframe[column_name] < lower_bound, lower_bound, dataframe[column_name])
    dataframe[column_name] = np.where(dataframe[column_name] > upper_bound, upper_bound, dataframe[column_name])

    return (
        dataframe,
        f"Ratio of values less than lower limit: {lower_outlier_ratio:.4f}",
        f"Ratio of values greater than upper limit: {upper_outlier_ratio:.4f}"
    )

def plot_feature_importance(model, features, num_features=10, save=False, filename="feature_importance.png"):
    """
    Plots the feature importance of a model.

    Parameters:
    ------
        model : object
            The trained model with feature_importances_ attribute (e.g., tree-based models).

        features : pd.DataFrame or list
            The features used to train the model.

        num_features : int, default= 10
            Number of top features to display. If None, all features are displayed.

        save : bool, optional
            If True, saves the plot as an image file.

        filename : str, default="feature_importance.png"
            The filename to use when saving the plot.

    Returns:
    ------
        None
    """
    feature_importance = pd.DataFrame({
        'Feature': features.columns if isinstance(features, pd.DataFrame) else features,
        'Importance': model.feature_importances_
    })

    feature_importance = feature_importance.sort_values(by="Importance", ascending=False)

    if num_features:
        feature_importance = feature_importance.head(num_features)

    plt.figure(figsize=(10, 6))
    sns.barplot(x="Importance", y="Feature", data=feature_importance, palette="flare")
    plt.title("Feature Importance")
    plt.tight_layout()

    if save:
        plt.savefig(filename)

    plt.show()

def plot_distribution(dataframe, column_name, plot= True):
  """
    Plots the distribution of a column in a given dataframe.

    Parameters
    ------
        dataframe: pd.DataFrame
            The dataframe containing the column to be plotted.

        column_name: str
            The name of the column to be plotted.

        plot: bool, optional
            If True, plots the distribution of the column.

    Returns
    ------
        dist_df: pd.DataFrame
            A dataframe containing the counts and percentages of each unique value in the specified column.

        None
        If plot is True, displays the distribution plot.
  """
  counts= dataframe[column_name].value(counts)
  percentages = 100 * dataframe[column_name].value_counts(normalize=True)

  dist_df = pd.DataFrame({column_name: counts, "Percentage": percentages})

    if plot:
        plt.figure(figsize=(8, 5))
        sns.barplot(x=dist_df.index, y="Percentage", data=dist_df, palette="rocket")
        plt.title(f"Distribution of {column_name}")
        plt.ylabel("Percentage")
        plt.xlabel(column_name)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

    return dist_df

def binary_label_encoder(dataframe):
  """
    It transfers the binary object columns to numeric binary variables.

    Parameters
    ------
        dataframe: pd.DataFrame
            The dataframe containing the binary object columns to be encoded.

    Returns
    ------
        dataframe: pd.DataFrame
            The dataframe with the binary object columns encoded as numeric binary variables.
  """
  labelencoder = LabelEncoder()
  binary_cols = [col for col in df.columns if df[col].dtypes == "O" and df[col].nunique() == 2]

  for col in binary_cols:
    dataframe[col] = labelencoder.fit_transform(dataframe[col])

  return dataframe

"""# **NLP**"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

def create_wordcloud_and_table(dataframe, text_column, num_words=20):
    """
      Generates a word cloud and a table of the most frequent words from a text column in a dataframe.

      Parameters:
      ------
          dataframe : pd.DataFrame
              The dataframe containing the text data.
          text_column : str
              The column name of the text data.
          num_words : int, optional
              Number of most frequent words to display in the table (default is 20).

      Returns:
      ------
          None
          Displays the word cloud and the frequency table.
    """

    text_data = ' '.join(dataframe[text_column].dropna().astype(str).values)

    wordcloud = WordCloud(width=200, height=100, background_color='white', colormap= 'inferno', normalize_plurals= False).generate(text_data)

    plt.figure(figsize=(8, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')
    plt.show()

    words = text_data.split()
    word_freq = Counter(words)
    most_common_words = word_freq.most_common(num_words)

    freq_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])

    print(freq_df)

def preprocess_text(text):
    """
      Preprocesses input text by removing stopwords, converting to lowercase,
      removing non-alphanumeric characters, and lemmatizing.

      Parameters:
      ------
        text : str
            The input text to be preprocessed.

      Returns:
      ------
        str
            The cleaned and preprocessed text.
    """

    text = text.lower()

    #Remove non-alphanumeric characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    tokens = word_tokenize(text)

    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    cleaned_text = ' '.join(tokens)

    return cleaned_text

"""# **Loss&Review**"""

def plot_loss(y_test, y_pred):
    """
      Prints the classification metrics for the given true and predicted values.

      Parameters:
      ------
        y_test : array-like
            True labels.
        y_pred : array-like
            Predicted labels.

      Returns:
      ------
        None
    """
    print(f"Accuracy: {round(accuracy_score(y_test, y_pred), 4)}")
    print(f"Recall: {round(recall_score(y_test, y_pred), 4)}")
    print(f"Precision: {round(precision_score(y_test, y_pred), 4)}")
    print(f"F1: {round(f1_score(y_test, y_pred), 4)}")
    try:
        print(f"Auc: {round(roc_auc_score(y_test, y_pred), 4)}")
    except ValueError as e:
        print(f"Auc: Not computable - {e}")

"""# **Examples for Functions**

- https://www.kaggle.com/datasets/muhammadehsan000/olympic-historical-dataset-1896-2020
"""

pip install kaggle

import warnings
warnings.filterwarnings("ignore")


import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

dataframe = pd.read_csv('/content/Olympic_Athlete_Event_Results.csv')
#/content/Olympic_Athlete_Event_Results.csv

